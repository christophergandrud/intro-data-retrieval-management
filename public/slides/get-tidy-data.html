<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Get and Clean Tidy Data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christopher Gandrud" />
    <script src="libs/header-attrs-2.13/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/robot.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Get and Clean Tidy Data
### Christopher Gandrud
### HU DYNAMICS
### 2020-05-22 (updated: 2022-04-24)

---




# üìù Lesson Preview

- What is public data?

- Retrieving and cleaning (tabular) public data

- APIs

- Cleaning grouped data

---

class: inverse, center, middle

# What is open public data?

---

# What is open public data?

Janssen (2012, 258):

"Non-privacy-restricted and non-confidential
data which is produced with public money and is made available without any
restrictions on its usage or distribution."

---

# Some benefits of open public data

- **Greater returns** on public investment in data gathering.

- Better **coordination** within government.

- Provides **policy-makers with information** needed to address complex problems.

- "**Mends the traditional separation** between public organizations and users".

See Janssen (2012, 261) for more.

---

# Mending separation between public and users

- **Assumes** that government **considers outside** views and information (including opposing) views as **constructive**.

  + Not always a correct assumption. E.g. there are reasons why it is basically impossible to get US-wide data on deaths by police.

- Results in government **giving up** (some) control and more **actively interacting**
with its environment.

---

# An ideal for open public data


&gt; Not only should data be published, but potential **data users in society** should be
**actively** sought for input on **improving government**.

---

# Challenges to open data

- **Lack of technological competence** to implement open data that is **useful**.

- **Worry** by bureaucrats that open data will be used to **criticise** them.

- **No incentives** for users to access the data. **Lack of skills** needed
to use and understand the data.

- Balancing individual **privacy** concerns.

See Janssen (2012, 262-263) for more.

---

# Recommended podcast

Max Ogden on the Request for Commits podcast discussing challenges faced working 
for the City of Boston with Code for America (&lt;https://changelog.com/rfc/6&gt;).

---

# Accessing data

Social science and public data is becoming **increasingly open** and **accessible**.

However, the level of **accessibility varies**:

- use restrictions

- format

- documentation

- version control

- data privacy restrictions

---

# So . . .

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

We are only going to begin **scratching the surface** of the data access
**obstacles** you are likely to encounter.

---

class: inverse, center, middle

# Retrieving and cleaning (tabular) public data

---

# Remember to programmatically tie your research to your data

Do as much **data gathering** and **cleaning** as possible programmatically:

- Fully document for reproducible research.

- Can find (inevitable) mistakes.

- Easy to update when the data is updated.

- Can apply methods to other data sets.

---

# "Easy" automatic data gathering

1. Plain-text data (e.g. CSV) stored at non-secure (http) URL, not embedded in
a larger HTML marked-up website.

2. Plain-text data (e.g. CSV) stored at secure (https) URL, not embedded in
a larger HTML marked-up website.

3. Data stored in a database with a well structured API (Application Programming
    Interface), that has a corresponding R package.

---

# Non-Secure URL Plain-text data

Use `read.table()` or `read.csv()` (just a wrapper for `read.csv()` with `sep = ','`).

Include the URL rather than the file path.


```r
read.table('http://SOMEDATA.csv')
```

---

# Loading compressed plain-text data

You can download and load data files stored in compressed formats (e.g. `.zip`).

1. Download the compressed file into a **temporary file**.

2. Uncompress the file and pass it to `read.table()`, `import()`, etc.

---

# Loading compressed plain-text data

Load data from @Pemstein2010 in a file called *uds_summary.csv*.


```r
# For simplicity, store the URL in an object called 'URL'.

URL &lt;- "https://bit.ly/3z4Lay2"

# Create a temporary file called 'temp' to put the zip file into.
temp &lt;- tempfile()

# Download the compressed file into the temporary file.
download.file(URL, temp)

# Decompress the file and convert it into a data frame
uds &lt;- read.csv(gzfile(temp, "uds_summary.csv"))

# Delete the temporary file.
unlink(temp)
```



---

# Secure (https) URL Plain-text data

Use `import()` from the *rio* package.

Data on GitHub is stored at secure URLs. Select the **RAW** URL:


```r
URL &lt;- paste0('https://raw.githubusercontent.com/christophergandrud/',
        'LegislativeViolence/master/Data/LegViolenceDescriptives.csv')
main &lt;- rio::import(URL)
```

---

# Versioning and reproducible research

Data maintainers (unfortunately) often change data sets with little or no documentation.

`source_data` allows you to notice these changes by assigning each file a unique
[SHA1 Hash](http://en.wikipedia.org/wiki/SHA-1).

Each download can be checked against the Hash:


```r
main &lt;- repmis::source_data(URL,
                sha1 = '01cff579b689cea9ef9c98e433ce3122745cc5cb')
```

```
## Downloading data from: https://raw.githubusercontent.com/christophergandrud/LegislativeViolence/master/Data/LegViolenceDescriptives.csv
```

```
## Specified SHA-1 hash matches downloaded data file.
```

üèä Dive deeper: [source code](https://github.com/christophergandrud/repmis/blob/3989ad8f5d6270ea1c939d8e53ad6c3c14afe4f1/R/utils.R#L39-L51) for hash check.

---

# Excel Files

The `source_XlsxData()` function in repmis does the same hash check as `source_data()`, but for Excel files. It builds on `read.xlsx()` for loading locally stored Excel files. 

Can also use `import()` from rio.

Note: Excel data often needs **a lot of cleaning** before it is useful
for statistical/graphical analyses.

---

# Caching

`source_data()` allows you to **cache** data with `cache = TRUE`
([source code](https://github.com/christophergandrud/repmis/blob/3989ad8f5d6270ea1c939d8e53ad6c3c14afe4f1/R/source_data.R#L84-L113)).

Caching stores data so that future requests for the data can be done faster.

Creates a locally stored version of the data set which is useful for 
reproducibility.

The cache and the source can become out of sync, hence the need to be able to compare their hashes.

There are other approaches to achieve similar goals, like [makefiles](https://www.gnu.org/software/make/manual/html_node/Introduction.html).

---

# Data APIs

API = Application Programming Interface, a documented way for programs to talk
to each other.

Data API = a documented way to access data from one program stored with another.

---

# R and Data APIs

R can interact with most data APIs using the [httr](https://github.com/hadley/httr)
package.

Even easier: users have written API-specific packages to interact with 
particular data APIs.

---

# World Bank Development Indications with WDI

Access the [World Bank's Development Indicators](http://data.worldbank.org/indicator)
with the WDI package.

Alternative Energy Use example:


```r
# Load WDI package
library(WDI)

# Per country alternative energy use as % of total energy use
alt_energy &lt;- WDI(indicator = 'EG.USE.COMM.CL.ZS')
```

Note: The indicator ID is at the end of the indicator's URL on the World Bank site.

---

# Financial Data with quantmod

The [quantmod](http://www.quantmod.com/) package allows you to access financial
data from a variety of sources (e.g. [Yahoo Finance](http://finance.yahoo.com/),
[Google Finance](https://www.google.com/finance),
[US Federal Reserve's FRED database](http://research.stlouisfed.org/fred2/)).


```r
library(quantmod)

# Download Yen/USD exchange rate
YenDollar &lt;- getSymbols(Symbols = 'DEXJPUS', src = 'FRED')

head(YenDollar)
```

```
## [1] "DEXJPUS"
```

---

# Other API-R packages

There are many more R packages that interact with web data APIs.

For a good beginner list see:
&lt;http://cran.r-project.org/web/views/WebTechnologies.html&gt;

---

# Working with non-table data

| Format                                      | R packages                                                              |
| ------------------------------------------- | ----------------------------------------------------------------------- |
| Excel, Stata, SPSS, SAS                     | rio                                                                     |
| [JSON](http://en.wikipedia.org/wiki/JSON)   | [jsonlite](https://cran.r-project.org/web/packages/jsonlite/index.html) |
| Databases, e.g. a Google BigQuery data base | [dbplyr](https://dbplyr.tidyverse.org/)                                 |

---

class: inverse, center, middle

# Data Cleaning

---

# üôáÔ∏è Remember: Tidy data semantics + structure

&lt;br&gt;

1. Each variable forms a column.

2. Each observation forms a row.

3. Each type of observational unit forms a table.

---

# üôáÔ∏è Remember: Tidy data

| Person       | treatment | result |
| ------------ | --------- | ------ |
| John Smith   | a         |        |
| Jane Doe     | a         | 16     |
| Mary Johnson | a         | 3      |
| John Smith   | b         | 2      |
| Jane Doe     | b         | 11     |
| Mary Johnson | b         | 1      |



---

# üôáÔ∏è Remember: Messy to Tidy data

First identify what your observations and variables are.

Then use R tools to convert your data into this format.

**tidyr** and is particularly useful.

---

# üôáÔ∏è Remember:Messy to tidy data

Using `pivot_longer()`


```r
tidied &lt;- tidyr::pivot_longer(data = messy, 
                     cols = c("a", "b"),     # Columns to pivot
                     names_to = "treatment",
                     values_to = "result")
tidied
```

```
## # A tibble: 6 √ó 3
##   person       treatment result
##   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;
## 1 John Smith   a             NA
## 2 John Smith   b              2
## 3 Jane Doe     a             16
## 4 Jane Doe     b             11
## 5 Mary Johnson a              3
## 6 Mary Johnson b              1
```

---

# üôáÔ∏è Remember: Tidy to messy data

Sometimes it is useful to reverse this operation with `pivot_wider`.


```r
messy_again &lt;- tidyr::pivot_wider(data = tidied, 
                                 names_from = "treatment",
                                 values_from = result)

messy_again
```

```
## # A tibble: 3 √ó 3
##   person           a     b
##   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;
## 1 John Smith      NA     2
## 2 Jane Doe        16    11
## 3 Mary Johnson     3     1
```

---

# üôáÔ∏è Remember: Other issues cleaning data

Always **look at** and **poke your data**.

For example, see if:

- Missing values are designated with `NA`

- Variable classes are what you expect them to be.

- Distributions are what you expect them to be.

[dlookr](https://cran.r-project.org/web/packages/dlookr/vignettes/diagonosis.html) looks promising for a data quality checker package.

---

# Join (merge) tidy data frames

Once you have tidy data frames, you can merge them for analysis.

In general: **each observation** must have a **unique key identifier** to merge them
on.

These identifiers **must match exactly across the data frames**.

---

# üîë Key-Value pairs

Key-value pairs are a fundamental data representation.

Data frames don't have explicit key-value pairs: e.g. 

$$
\langle \mathrm{Christopher},\: \mathrm{Lecturer}\rangle
$$

`Christopher` is the key and the value is `Lecturer`. 

ü§ì Fun fact: this is also called a `2-tuple`. Other languages like Python and Julia handle these with data structures called "dictionaries".

---

# Data frames!

Data frames don't have explicit key-value pairings

- Nice for flexibility

- Bad for data manipulation speed and consistency  

---

# Merging data


```r
tail(alt_energy, n = 3)
```

```
##       iso2c  country EG.USE.COMM.CL.ZS year
## 16224    ZW Zimbabwe                NA 1962
## 16225    ZW Zimbabwe                NA 1961
## 16226    ZW Zimbabwe                NA 1960
```

```r
tail(uds, n = 3)
```

```
##         X       country year cowcode      mean        sd    median     pct025
## 9135 9135 Western Samoa 2006     990 0.2485397 0.2155926 0.2474691 -0.1702781
## 9136 9136 Western Samoa 2007     990 0.2439135 0.2151686 0.2487054 -0.1874372
## 9137 9137 Western Samoa 2008     990 0.2407623 0.2192563 0.2443505 -0.1881970
##         pct975
## 9135 0.6648156
## 9136 0.6571918
## 9137 0.6622615
```

---

# Create unique ID: country codes

Unique identifier will be
[iso 2 letter country code](http://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)
and **year**.

Use the [countrycode](https://github.com/vincentarelbundock/countrycode)
package to turn UDS data's
[Correlates of War Country Code](http://www.correlatesofwar.org/datasets.htm)
(`cowcode`) to `iso2c`.


```r
library(countrycode)

# Assign iso2c codes base on correlates of war codes
uds$iso2c &lt;- countrycode(uds$cowcode, origin = 'cown',
                             destination = 'iso2c', warn = TRUE)
```

**NOTE**: Always check the data to make sure the correct codes have been applied!

---

# Creating IDs: geocodes

countrycode clearly only works for standardising country IDs

Other packages can be useful for standardising other unit IDs.

For example, `geocode` from
[ggmap](http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf) can be
used to create latitude/longitudes for other geographic units:


```r
places &lt;- c('Bavaria', 'Seoul', '6 Parisier Platz, Berlin')
ggmap::geocode(places, source = 'google')

##         lon      lat
## 1  11.49789 48.79045
## 2 126.97797 37.56654
## 3  13.37854 52.51701
```

Note: [you need to](https://github.com/dkahle/ggmap#attention) sign up for a Google Cloud account to use this.

---

# Creating IDs: Time 

Time units may be important components of observation IDs.

Use the [lubridate](http://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html)
package to standardise dates.

---

# Creating IDs: Time


```r
library(lubridate)

# Create time data
times &lt;- c('Sep. 17 1980', 'March 23 2000', 'Nov. 3 2003')
mdy(times)
```

```
## [1] "1980-09-17" "2000-03-23" "2003-11-03"
```

Note: Times should always go from **longest to shortest** unit (e.g. YEAR-MONTH-DAY). 
Makes dates **sortable**.

---

# Join data


```r
# Keep only desired variables
uds &lt;- uds[, c('iso2c', 'year', 'median')]
names(uds)
```

```
## [1] "iso2c"  "year"   "median"
```


```r
combined &lt;- full_join(x = alt_energy, 
                      y = uds,
                      by = c('iso2c', 'year'))
head(combined, n = 3)
```

```
##   iso2c                     country EG.USE.COMM.CL.ZS year median
## 1    ZH Africa Eastern and Southern                NA 2020     NA
## 2    ZH Africa Eastern and Southern                NA 2019     NA
## 3    ZH Africa Eastern and Southern                NA 2018     NA
```

---

# (Many) of the joins


| Name         | Description                                               |
| ------------ | --------------------------------------------------------- |
| `full_join`  | Returns all rows and columns from both data sets          |
| `inner_join` | Returns all rows where there is a match in both data sets |
| `left_join`  | Returns all rows from x                                   |
| `right_join` | Returns all rows from y                                   |

Joins are a crucial part of Structure Query Language (**SQL**) work that is common in commercial data science for working with data bases

Always **check your data** after a merge to see if you did what you wanted to do!

---

# Clean up

You many want to do some post merge cleaning. For example assign new variable
names:


```r
names(combined) &lt;- c('iso2c', 'year', 'country',
                     'alt_energy_use',  'uds_median')
```

or


```r
combined &lt;- dplyr::rename(combined, new_year = year)
```

---

# Reorder variables


```r
combined &lt;- dplyr::relocate(combined, country)

head(combined)
```

```
##                       country iso2c EG.USE.COMM.CL.ZS new_year median
## 1 Africa Eastern and Southern    ZH                NA     2020     NA
## 2 Africa Eastern and Southern    ZH                NA     2019     NA
## 3 Africa Eastern and Southern    ZH                NA     2018     NA
## 4 Africa Eastern and Southern    ZH                NA     2017     NA
## 5 Africa Eastern and Southern    ZH                NA     2016     NA
## 6 Africa Eastern and Southern    ZH                NA     2015     NA
```

---

# More data transformations with dplyr

Set up for examples


```r
# Create fake grouped data
library(randomNames)
library(dplyr)
library(tidyr)

people &lt;- randomNames(n = 1000)
people &lt;- sort(rep(people, 4))
year &lt;- rep(2010:2013, 1000)
trend_income &lt;- c(30000, 31000, 32000, 33000)

income &lt;-  replicate(trend_income + rnorm(4, sd = 20000),
                     n = 1000) %&gt;%
            data.frame() %&gt;%
            gather(obs, value, X1:X1000)

income$value[income$value &lt; 0] &lt;- 0
data &lt;- data.frame(people, year, income = income$value)
```

---

# dplyr


```r
head(data)
```

```
##            people year   income
## 1 Ablan, Schuyler 2010 42243.40
## 2 Ablan, Schuyler 2011 35809.04
## 3 Ablan, Schuyler 2012 27518.97
## 4 Ablan, Schuyler 2013 50316.70
## 5  Abraha, Tamara 2010 70656.34
## 6  Abraha, Tamara 2011 21004.43
```

---

# Filter dplyr

Select rows


```r
higher_income &lt;- filter(data, income &gt; 60000)

head(higher_income)
```

```
##                 people year   income
## 1       Abraha, Tamara 2010 70656.34
## 2        Adams, Collin 2011 70515.79
## 3     al-Abbas, Farhat 2010 62407.17
## 4 al-Abdallah, Zubaida 2013 66061.82
## 5      al-Abu, Hameeda 2012 62070.93
## 6      al-Amber, Hilmi 2010 87288.64
```

---

# Select dplyr

Select columns


```r
people_income &lt;- select(data, people, income)

# OR
people_income &lt;- select(data, -year)

head(people_income)
```

```
##            people   income
## 1 Ablan, Schuyler 42243.40
## 2 Ablan, Schuyler 35809.04
## 3 Ablan, Schuyler 27518.97
## 4 Ablan, Schuyler 50316.70
## 5  Abraha, Tamara 70656.34
## 6  Abraha, Tamara 21004.43
```

---

# dplyr with grouped data

Tell dplyr what the groups are in the data with `group_by`.


```r
group_data &lt;- group_by(data, people)

head(group_data)[1:5, ]
```

```
## # A tibble: 5 √ó 3
## # Groups:   people [2]
##   people           year income
##   &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;
## 1 Ablan, Schuyler  2010 42243.
## 2 Ablan, Schuyler  2011 35809.
## 3 Ablan, Schuyler  2012 27519.
## 4 Ablan, Schuyler  2013 50317.
## 5 Abraha, Tamara   2010 70656.
```

Note: the following functions work on **non-grouped data** as well.

---

# dplyr with grouped data

Now that we have declared the data as grouped, we can do operations on each group.

For example, we can extract the highest and lowest income years for each person:


```r
min_max_income &lt;- summarize(group_data,
                            min_income = min(income),
                            max_income = max(income))

head(min_max_income)[1:3, ]
```

```
## # A tibble: 3 √ó 3
##   people          min_income max_income
##   &lt;chr&gt;                &lt;dbl&gt;      &lt;dbl&gt;
## 1 Ablan, Schuyler     27519.     50317.
## 2 Abraha, Tamara      19474.     70656.
## 3 Acosta, Mariah      27928.     38364.
```

---

# dplyr with grouped data

We can sort the data using `arrange`.


```r
# Sort highest income for each person in ascending order
ascending &lt;- arrange(min_max_income, max_income)

head(ascending)[1:3, ]
```

```
## # A tibble: 3 √ó 3
##   people             min_income max_income
##   &lt;chr&gt;                   &lt;dbl&gt;      &lt;dbl&gt;
## 1 Adams, Emily            4130.     12956.
## 2 el-Greiss, Hishaam         0      17092.
## 3 Lange, May                 0      19442.
```

---

# dplyr with grouped data

Add `desc` to sort in descending order


```r
descending &lt;- arrange(min_max_income, desc(max_income))

head(descending)[1:3, ]
```

```
## # A tibble: 3 √ó 3
##   people         min_income max_income
##   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;
## 1 Sanchez, Noely     18638.    111425.
## 2 Garcia, Josiah     24863.    103468.
## 3 Reaves, Naim        6359.     93859.
```

---

# dplyr with grouped data

`summarize` creates a new data frame with the summarised data.

We can use `mutate` to add new columns to the original data frame.


```r
data &lt;- mutate(group_data,
                min_income = min(income),
                max_income = max(income))

head(data)[1:3, ]
```

```
## # A tibble: 3 √ó 5
## # Groups:   people [1]
##   people           year income min_income max_income
##   &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1 Ablan, Schuyler  2010 42243.     27519.     50317.
## 2 Ablan, Schuyler  2011 35809.     27519.     50317.
## 3 Ablan, Schuyler  2012 27519.     27519.     50317.
```

---

class: inverse, center, middle
background-color: #FF8C03

# ü•Ö Practice

Download two data sets (that conceivable could be merged) using the API-based packages discussed today. Either in the same or a linked R script  **clean** and **merge** the data.


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
