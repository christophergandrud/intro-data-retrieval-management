---
title: "Parse PDFs and Text as Data"
author: "Christopher Gandrud"
institute: "HU DYNAMICS"
date: "2022-06-10 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [robot, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
bibliography: main.bib
---

```{r setup, include=FALSE}
xfun::pkg_attach2("pdftools", "tidyverse", "rvest", "httr", "roperators",
                  "quanteda", "quanteda.textplots", "xml2")
knitr::opts_chunk$set(fig.align = 'center')
```

## üìù Lesson Preview

- Scraping tables in PDFs

- Text as data

---

class: inverse, center, middle
background-color: #FFC400

# Scraping tables in PDFs

---

class: inverse, center, middle
background-color: #FB3579

## "Please send the data in a PDF" -- *no one ever*

---

# Table in a PDF

```{r peace-table, echo=FALSE, out.height="300px", out.width="400px", fig.align='center'}
knitr::include_graphics("img/peace-table.png")
```

PDF at: <https://www.visionofhumanity.org/wp-content/uploads/2020/10/GPI_2020_web.pdf>

Example modified from [Ketan Deshpande](https://medium.com/@ketanrd.009/how-to-extract-pdf-tables-in-r-e994c0fe4e28) and then updated to use Python.

---

# parse PDF 

There used to an R package called [tabulizer](https://github.com/ropensci/tabulizer) package to parse PDFs in R. But it is not currently downloadable on CRAN.

There are at least two alternatives:

- (the one I don't recommend) [pdftools](https://docs.ropensci.org/pdftools/) package to parse PDFs in R. This does not require rJava, but its `pdf_data()` function is less powerful for extracting tables from PDFs.

- tabula-py: a Python version of tabulizer. It's more powerful than pdftools, but harder to set up. It requires setting up Java, Python, and calling the two from R. See set up instructions [here](https://github.com/christophergandrud/hu-live-coding-course-examples/blob/main/src/set-up-tabula-python.md).

---

# Download PDF and extract table

```{r download-pdf, cache=TRUE, warning=FALSE, message=FALSE, eval=FALSE}
xfun::pkg_attach2("tidyverse", "reticulate")

# Find the the location of Python on your computer with `which python3` in the Terminal
use_python("/opt/homebrew/bin/python3")

# Load tabula package
tabula <- import("tabula")

table_list <- tabula$read_pdf("https://www.visionofhumanity.org/wp-content/uploads/2020/10/GPI_2020_web.pdf", 
                        user_agent="Mozilla/5.0", pages="10")

df <- table_list[[1]] # Extract data table from list
head(df)
```

[GitHub](https://github.com/christophergandrud/hu-live-coding-course-examples/blob/main/src/tabula-reticulate-example.R)

The data frame still needs cleaning using the tools we covered in previous sessions.

Note: you can also extract text data from PDFs using tabula.

---

class: inverse, center, middle
background-color: #FFC400

# üìë Text as data

---

# üìë Text as data

So far we have assumed that we want to get data into tidy data format, probably for a standard statistical analysis (e.g. linear regression). 

But, we may want to conduct an analysis of the text itself, e.g. [Grimmer and Stewart (2013)](https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20), using **natural language processing** (NPL) methods. 

Key tools include the [quanteda](https://github.com/quanteda/quanteda) and [tm](https://cran.r-project.org/web/packages/tm/tm.pdf) (text mining) packages. 

---

## Large Language Models (LLMs) and Text Analysis

LLMs not only have the potential to improve our coding practice, but also
have considerable potential for analysing texts:

-   high level of "understanding"

-   flexible for texts

-   texts still require cleaning, but likely less (e.g. don't have to remove
"stopwords")

‚ö†Ô∏è But there are pitfalls, including:

-   **challenging reproducibility**, especially for proprietary models like GPT-4

-   computational expense, especially for long texts (though should improve 
over time with [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law)-type improvements) 


---

# Example

Let's scrape COVID-19 press releases from the British Government.

Navigate to <https://www.gov.uk/search/news-and-communications>.

```{r uk-gove-website, echo=FALSE, out.height="400px", out.width="400px"}
knitr::include_graphics("img/uk-gov-announcements.png")
```

---

# Checkout URL structure

Imagine we want to study the Prime Minister's communications regarding COVID-19. 

Understand the URL structure:

```{r boris-news, echo=FALSE, out.height="200px", out.width="200px"}
knitr::include_graphics("img/boris-news.png")
```

<https://www.gov.uk/search/news-and-communications?level_one_taxon=5b7b9532-a775-4bd2-a3aa-6ce380184b6c&people%5B%5D=boris-johnson&order=updated-newest>

---

# Checkout URL structure for announcements


```{r boris-lists, echo=FALSE}
knitr::include_graphics("img/list-item.png")
```

Links to each announcement are stored in the `href` of anchor tags (`<a>`).

---

# Download the search page

```{r download-announce}
library(rvest)

url <- "https://www.gov.uk/search/news-and-communications?level_one_taxon=5b7b9532-a775-4bd2-a3aa-6ce380184b6c&people%5B%5D=boris-johnson&order=updated-newest"

announce_page <- read_html(url)

announce_page
```

---

# Extract the partial URLS with xpath

**xpath** (XML Path Language) allows you to query nodes in an html document.

```{r xpath-copy, echo=FALSE}
knitr::include_graphics("img/xpath-copy.gif")
```

---

# Extract the partial URLs with xpath

```{r}
announce_page_href_1 <- html_elements(announce_page,
                        xpath = '//*[@id="js-results"]/div/ul/li[1]/a') %>%
                        html_attr('href') # we want the href element
announce_page_href_1
```

<br>
<br>

This only extracted the first href because we explicitly referenced the first list item (`li[1]`)
---

# üî• Wild cards

Use the `*` wild card to match all of the list items. 

```{r}
announce_page_href <- html_elements(announce_page,
                                 xpath = '//*[@id="js-results"]/div/ul/li[*]/a') %>%
  html_attr('href') # we want the href element
announce_page_href
```

---

# Multiple pages

Notice:

```{r}
length(announce_page_href)
```

We are missing the additional pages.

Notice URL `page` reference: <https://www.gov.uk/search/news-and-communications?level_one_taxon=5b7b9532-a775-4bd2-a3aa-6ce380184b6c&order=updated-newest&page=2&people%5B%5D=boris-johnson>

---

# While loops to download multiple pages

```{r}
xfun::pkg_attach2("roperators") # use infix operator %+=%

all_href <- character() # initialise all results object
tmp_href <- 1 # initalise temp object
i <- 0 # initalise counter
while (length(tmp_href) > 0 ) {
  i %+=% 1
  url <- paste0("https://www.gov.uk/search/news-and-communications?",
                "level_one_taxon=5b7b9532-a775-4bd2-a3aa-6ce380184b6c&",
                "order=updated-newest&page=", 
                as.character(i), 
                "&people%5B%5D=boris-johnson")

  tmp_href <- read_html(url) %>%
              html_elements(xpath = '//*[@id="js-results"]/div/ul/li[*]/a') %>%
                         html_attr('href')
  
  all_href <- c(all_href, tmp_href)
}
```

---

# While loops to download multiple pages

```{r}
length(all_href)

all_href
```

---

# Get text from each page

```{r}
url_one <- paste0("https://www.gov.uk", all_href[1])

url_one
```

---

# Get text from each page

```{r one-announce-text, echo=FALSE}
knitr::include_graphics("img/one-announce-xpath.gif")
```

---

# Get text from each page

```{r, cache=TRUE}
one_announcement <- read_html(url_one) %>%
          html_nodes(xpath = '//*[@id="content"]/div[3]/div[1]/div[1]/div') %>%
          html_text()

one_announcement
```

---

# Get text from all pages

```{r}
all_texts <- data.frame()
for (i in seq(all_href)) {
  url_one <- paste0("https://www.gov.uk", all_href[i])
  message(url_one)
  
  text <- read_html(url_one) %>%
      html_nodes(xpath = '//*[@id="content"]/div[3]/div[1]/div[1]/div') %>%
      html_text()
  tmp_df <- data.frame(url = all_href[i], text = text)
  all_texts <- bind_rows(all_texts, tmp_df)
}

nrow(all_texts)
```

---

# Simple analysis

Convert to a corpus object

```{r}
xfun::pkg_attach2("quanteda")
announce_corpus <- corpus(all_texts)

summary(announce_corpus)
```

---

# A problem

None of the documents should have only one sentence.

üêõ We need to debug.

```{r echo=FALSE}
knitr::include_graphics("img/govspeach.png")
```

---

# xpath [contains](https://developer.mozilla.org/en-US/docs/Web/XPath/Functions/contains) function

```{r}
all_texts <- data.frame()
for (i in seq(all_href)) {
  url_one <- paste0("https://www.gov.uk", all_href[i])
  # message(url_one)
  full_html <- read_html(url_one)
  
  text <-  full_html %>% 
    html_nodes(xpath = 
      paste0('//*[@id="content"]/div[3]/div[1]/div',
             '/div[1]/div[contains(@data-module, "govspeak")]')) %>% 
    html_text()
  
  tmp_df <- data.frame(url = all_href[i], text = text)
  all_texts <- bind_rows(all_texts, tmp_df)
}

announce_corpus <- corpus(all_texts)

summary(announce_corpus)
```

---

# Create a document-term matrix

```{r}
announce_dtm <- announce_corpus %>%
                  tokens(remove_punct = TRUE) %>%
                  dfm() %>%
                  dfm_remove(stopwords("english")) %>%
                  dfm_wordstem() 

announce_dtm
```

---

## The "mullet" of text analysis

```{r, warning=FALSE, cache=FALSE}
xfun::pkg_attach2("quanteda.textplots")
textplot_wordcloud(announce_dtm, min_count = 9)
```

---

class: inverse, center, middle
background-color: #FF8C03

# ü•Ö Practice

Scrape and clean up the first table from <https://www.visionofhumanity.org/wp-content/uploads/2020/10/GPI_2020_web.pdf>

---

class: inverse, center, middle
background-color: #FF8C03

# ü•Ö Pair Project

With a partner, find an online data source. 

What research question might you answer with this data set?

Scrape and clean it for analysis.

Bonus: [sign up for ChatGPT](https://chat.openai.com/auth/login) and try incorporating it into your analysis process. 

-   What process is most effective?

-   When did it help? When was it not useful?
- 