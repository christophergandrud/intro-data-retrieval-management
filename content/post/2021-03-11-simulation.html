---
title: '[Intro to Computational Thinking] Introduction to Computational Thinking via Simulation with R'
author: Christopher Gandrud
date: '2023-05-15'
slug: optional-r-basics-3
weight: 4
categories:
  - R
  - install
tags: [R, simulation, distributed, functions]
Categories: []
Description: ''
Tags: [R, simulation, distributed, functions]
bibliography: main.bib
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/d3/d3.min.js"></script>
<script src="/rmarkdown-libs/dagre/dagre-d3.min.js"></script>
<link href="/rmarkdown-libs/mermaid/dist/mermaid.css" rel="stylesheet" />
<script src="/rmarkdown-libs/mermaid/dist/mermaid.slim.min.js"></script>
<link href="/rmarkdown-libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="/rmarkdown-libs/chromatography/chromatography.js"></script>
<script src="/rmarkdown-libs/DiagrammeR-binding/DiagrammeR.js"></script>


<blockquote>
<p>‚ÄúWhat I cannot create, I do not understand‚Äù ‚Äì <a href="https://archives.caltech.edu/pictures/1.10-29.jpg">Richard Feynman</a></p>
</blockquote>
<blockquote>
<p>‚ÄúIt is immoral to collect data before simulating the hypothesized data generating process.‚Äù ‚Äì Andrew Gelman (what I remember of a comment to the Stanford Causal Inference Seminar in 2020).</p>
</blockquote>
<pre class="r"><code># Load required packages
xfun::pkg_attach2(&quot;DiagrammeR&quot;)</code></pre>
<div id="lesson-preview" class="section level2">
<h2>üìù Lesson Preview</h2>
<ul>
<li><p><a href="#motivation">Why simulate?</a></p></li>
<li><p><a href="#workflow">Simple simulation workflow</a></p></li>
<li><p><a href="#documenting">Documenting Functions in R</a></p></li>
<li><p><a href="#monte-carlo">Monte Carlo with apply and parallelization</a></p></li>
<li><p><a href="#debugging-intro">Debugging in R</a></p></li>
<li><p>Throughout, we will include examples of how to plot with base R and ggplot2</p></li>
</ul>
</div>
<div id="motivation" class="section level2">
<h2>Motivation: Why simulate?</h2>
<p>We‚Äôve covered the basics of programming in R. Let‚Äôs bring many of these concepts together to help us better understand our research problems, even before we gather any data. To do this, we‚Äôll learn how to use R to simulate and analyse <strong>data generating processes (DGP)</strong>.</p>
<p>A data generating process is the process that creates the data in a data set. These processes can include, for example, both the social processes that result in migration data and even unexpected (or undesired) issues in the sampling procedure like non-random study attrition.</p>
<p>Why are we talking about simulating data generating processes in an introduction to R course? There are two sets of reasons.</p>
<p>First, computational generative modeling‚Äìsimulating data generating processes with a computer‚Äìis a useful <strong>tool for understanding and evaluating the statistical methods</strong> you are using to study data generating processes <span class="citation">(<a href="#ref-gelmanhillaki2021">Gelman, Hill, and Vehtari 2021, 76</a>)</span>. Will your chosen statistical model identify the effect you are interested, even if you completely understand the DGP? This is surprisingly often not the case, especially for non-trivial data generating processes. Simulation allows you to stress test your models, with a given assumed DGP, and compare the appropriateness of different models for this DGP.</p>
<p>In Gelman‚Äôs strong view stated above, if your identification strategy can‚Äôt recover effects in known‚Äìsimulated data‚Äìyou shouldn‚Äôt waste time and money collecting real world data. If data gathering is time consuming and expensive, you can also use generative modelling to make progress on your problem before your data is fully collected.</p>
<p>The second reason we are covering statistical simulation, and most importantly for this class, is that simulation requires you to <strong>work your new R programmering skills</strong>. The DGP we will work with are probabilistic. You need to repeat the simulations with the same data generating process multiple times to understand its statistical properties. This is called the <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo Method</a>. You should never do this repeated sampling by rewriting the same function over and over. Instead, encode the sampling in a function that you can run repeatedly and, ideally, efficiently, e.g.¬†in parallel.</p>
</div>
<div id="workflow" class="section level2">
<h2>Simple simulation workflow</h2>
<p>This is a typical simulation workflow:</p>
<div class="DiagrammeR html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-1" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"\ngraph TB\n  A[Hypothesise DGP]-->AA[Identification strategies]\n  AA-->B[Functions for 1 draw & analysis]\n  B-->C[Function to repeat for n draws]\n  B-- Debug -->B\n  C-->D[Plot simulations]\n  C-- Debug -->C \n  D-- Debug -->D\n  D-- Iterate -->A\n"},"evals":[],"jsHooks":[]}</script>
<p>We start by hypothesizing what the data generating process we care about might be. Then we propose one or more candidate strategies to identify effects in this data generating process, e.g.¬†regression, matching, difference in differences. We begin implementing the simulation for the DGP-identification strategy by creating function(s) for simulating and analyzing the data for <em>one simulation draw</em>. Once we have the simulation correctly working for one draw, we create a function(s) to make <span class="math inline">\(n\)</span> additional draws. We then (usually) examine the results by plotting quantities of interest from the simulations. Quantities of interest could be the distribution of some estimate we are interested in or the error from some ‚Äúground truth‚Äù across simulations. Frequently, we learn something unexpected by the end of this process. Our learnings could include a new understanding of the data generating process or our candidate identification strategies. Because we often learn something about how our originally selected identification strategy is lacking or our proposed DGP is too simplistic, we often need to repeat the process a number of times before we are confident we understand even the simulated data and its hypothetical relationship to the real social phenomenon we are interested in.</p>
<p>Notice that most steps can involve some debugging. We‚Äôll dig into debugging <a href="#debugging-intro">below</a>. But let‚Äôs look at a simple ideal case first.</p>
<div id="example-1-simple-ab-test-discrete-probabilities" class="section level3">
<h3>Example 1: Simple A/B Test Discrete probabilities</h3>
<p>Imagine that we want to run an A/B test‚Äìrandomised control trial‚Äìto evaluate the causal effect of a new treatment relative to the status quo control counterfactual. We are interested in the effect of the treatment on whether or not individuals in the study do some action‚Äìe.g.¬†vote in an election.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Imagine the outcome is a discrete variable coded <span class="math inline">\([0, 1]\)</span>. Can we use a difference of means test to estimate the treatment effect?</p>
<p>Let‚Äôs start by hypothesizing what the data generating process could be. Imagine that based on prior research we know that there is a baseline probability of the outcome being <span class="math inline">\(1\)</span> when a person is exposed to the control of 0.1, i.e.:</p>
<p><span class="math display">\[Y \sim \mathrm{Binomial}(0.1)\]</span> To simulate one draw from this binomial distribution in R use:</p>
<pre class="r"><code>control_y_1 &lt;- rbinom(n = 1, size = 1, prob = 0.1)
control_y_1</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Let‚Äôs walk through this code. <code>rbinom()</code> is the function to randomly (the <code>r</code> in <code>rbinom</code>) draw from the binomial distribution (<code>binom</code>). <code>n</code> is the number of draws for this simulation, <code>size</code> is the ‚Äúnumber of trials‚Äù. Basically it means that you want the ‚Äúsuccess‚Äù outcome to be 1 and 0 otherwise. <code>prob</code> is the probability of ‚Äúsuccess‚Äù.</p>
<p>It‚Äôs unlikely that we will run an A/B test with only one participant in the control group (this would be hilariously under-powered). So, let‚Äôs simulate a test with 2,000,000 draws<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> and plot the results.</p>
<pre class="r"><code># number of observations variable to ensure consistency across sims
n_obs &lt;- 2e6
control_prob &lt;- 0.1

control_y &lt;- rbinom(n = n_obs, size = 1, prob = control_prob)
hist(control_y)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<hr />
</div>
<div id="side-note-histograms-with-ggplot2" class="section level3">
<h3>Side note: Histograms with ggplot2</h3>
<p><a href="https://ggplot2.tidyverse.org/">ggplot2</a> is a very powerful way to create graphics in R. Here is an example of how you would create a histogram with ggplot2.</p>
<pre class="r"><code># ggplot2 is a member of the &quot;tidyverse&quot; set of packages
xfun::pkg_attach2(&quot;tidyverse&quot;)

# I personally enjoy the simplicity of the &quot;linedraw&quot; theme
theme_set(theme_linedraw())

# ggplot cannot accept a vector, like `hist` 
# You need to convert to a data frame or the tidyverse equivalent &quot;tibble&quot;
control_y_tbl &lt;- as_tibble(control_y)

# Plot and change the x-axis label
ggplot(control_y_tbl, aes(x = value)) + # as_tibble names the column &quot;value&quot; by default
  geom_histogram() + 
  xlab(&quot;\nControl Outcome&quot;) + # \n adds a line break before the label
  ggtitle(&quot;Simple Simulation Histogram&quot;, subtitle = &quot;Control Group&quot;)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Notice how you build up the layers of the plot in <em>ggplot</em> by adding ‚Äúgeoms‚Äù with plus (<code>+</code>) notation. These are literally stacked on top of each other in the order you enter them in code. So the base layer is set with <code>ggplot</code>. This specifies the data frame to work with and the aesthetics (<code>aes</code>) layer that defines things like the X and Y coordinates, and grouping variables. Histograms only need the x-axis specified in <code>aes</code> because the y-axis will be the automatically generated count per histogram bin. But for other geoms, such as <code>geom_point</code> for scatter plots, you also need to specify a variable that defines the y-axis with the <code>y</code> argument.</p>
<p>If you plan to make similar ggplot plots multiple times, consider wrapping it in a function. For example:</p>
<pre class="r"><code># @param outcomes numeric vector
# @param subtitle character string of the plot&#39;s subtitle

plot_histogram &lt;- function(outcomes, subtitle) {
  if (missing(subtitle)) stop(&quot;subtitle must be specified&quot;)
  x_tbl &lt;- as_tibble(outcomes)
  ggplot(x_tbl, aes(x = value)) + 
  geom_histogram() + 
  xlab(&quot;\nControl Outcome&quot;) +
  ggtitle(&quot;Simple Simulation Histogram&quot;, subtitle = subtitle)
}

plot_histogram(outcomes = control_y, subtitle = &quot;Control Group&quot;)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<hr />
<p>Ok, we‚Äôve simulated one run of the control group. Now let‚Äôs think about the treatment group. Imagine we anticipate that a realistic effect of the treatment‚Äìagain based on prior knowledge‚Äìis an average relative improvement of 1%. We simulate this with:</p>
<pre class="r"><code>rel_effect &lt;- 0.01
treat_prob &lt;- control_prob + (control_prob * rel_effect)
treatment_y &lt;- rbinom(n = n_obs, size = 1, prob = treat_prob)

plot_histogram(treatment_y, subtitle = &quot;Treatment Group&quot;)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Finally, we compare the means of the two groups:</p>
<pre class="r"><code>mean(treatment_y) - mean(control_y) </code></pre>
<pre><code>## [1] 0.000808</code></pre>
<p>That‚Äôs one simulation. Let‚Äôs wrap it up in a function so it will be easy to run <span class="math inline">\(n\)</span> simulations</p>
<pre class="r"><code># @param n integer number of draws per treatment arm
# @param control_prob numeric probability of control success [0, 1]
# @param rel_effect numeric relative effect, change in success probability 
# caused by treatment 

one_sim &lt;- function(n, control_prob, rel_effect) {
  treat_prob &lt;- control_prob + (control_prob * rel_effect)
    
  cy &lt;- rbinom(n = n, size = 1, prob = control_prob)
  ty &lt;- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}</code></pre>
<p>Let‚Äôs test it to make sure it works.</p>
<pre class="r"><code>one_sim(n = n_obs, control_prob = control_prob, 
        rel_effect = rel_effect)</code></pre>
<pre><code>## [1] 0.0009145</code></pre>
<p>This is very close to the true increase absolute of <code>0.001</code> (i.e.¬†1% of the <code>control_prob</code> of 0.1).</p>
<hr />
</div>
</div>
<div id="documenting" class="section level2">
<h2>Side note: Documenting Functions in R</h2>
<p>It is important to get in the habit of documenting your functions as you write them. This makes them easier for others (and yourself in the future) to understand.</p>
<p>There are some R documentation conventions to follow.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Function documentation begins with a description of what the function does. Then each function parameter is documented using <code>@param</code>. It is not required, but it is good practice to document the expected type of the value for each argument, e.g.¬†<code>integer</code>. You can also give examples of the function working with <code>@examples</code>. All of the documentation should be commented out with <code>#'</code>. E.g.</p>
<pre class="r"><code>#&#39; Simulate one A/B test outcome from the binomial distribution
#&#39; and return the difference of the means of the two treatment arms.
#&#39; 
#&#39; @param n integer number of observations per treatment arm
#&#39; @param control_prob numeric probability of success in the control group
#&#39; @param rel_effect numeric relative effect of the treatment compared
#&#39; to the control

one_sim &lt;- function(n, control_prob, rel_effect) {
  treat_prob &lt;- control_prob + (control_prob * rel_effect)
    
  cy &lt;- rbinom(n = n, size = 1, prob = control_prob)
  ty &lt;- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}</code></pre>
<hr />
<div id="monte-carlo" class="section level3">
<h3>Monte Carlo with <code>for</code></h3>
<p>Now that we‚Äôve created a function for one total draw of the simulated A/B test, let‚Äôs repeatedly sample it so that we can better understand the probabilistic properties of our hypothesized data generating process.</p>
<p>One way to do this is with <code>for</code> loops. For example:</p>
<pre class="r"><code># package to add a progress bar to monitor loop progress
xfun::pkg_attach2(&quot;progress&quot;)

# Create vector of NAs to take the simulation results
n_sims &lt;- 100
mean_diff &lt;- rep(NA, n_sims)
pb &lt;- progress_bar$new(total = n_sims)

for (i in seq_along(mean_diff)) {
  pb$tick() # increment the progress bar
  mean_diff[i] &lt;- one_sim(n = n_obs, control_prob = control_prob, 
        rel_effect = rel_effect)
  #if (is.na(mean_diff[i])) stop(paste(i, &quot;is missing&quot;))
}

hist(mean_diff)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="monte-carlo-with-apply-and-parallelization" class="section level3">
<h3>Monte Carlo with <code>apply</code> and parallelization</h3>
<p><code>for</code> loops are <a href="https://privefl.github.io/blog/why-loops-are-slow-in-r/">notoriously slow in R</a>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This isn‚Äôt a problem if you are only doing a few simulations, but gets very tedious if you need to run many simulations. Often‚Äìthough not always‚Äìusing the <code>apply</code> family of functions is faster. They also can make it easier to parallelize your code.</p>
<p>Since we want to run the same function <code>n</code> times, we can use the <code>replicate()</code> helper function for the <code>apply</code> family:<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<pre class="r"><code>system.time(
  mean_diff &lt;- replicate(n_sims, one_sim(n = n_obs, 
                                       control_prob = control_prob, 
                                       rel_effect = rel_effect))
)</code></pre>
<pre><code>##    user  system elapsed 
##   5.657   0.139   5.796</code></pre>
<p>I wrapped this call in <code>system.time()</code> so that we can keep track of how much computation time we spend on the call. Monte Carlo simulations are just doing the same thing over and over. This computation time can add up.</p>
<p>We can see that the the <code>apply</code> version (above) in this case is pretty much as slow as the <code>for</code> loop (below).<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<pre class="r"><code>system.time(
  for (i in seq_along(mean_diff)) {
    mean_diff[i] &lt;- one_sim(n = n_obs, control_prob = control_prob, 
        rel_effect = rel_effect)
  }
)</code></pre>
<pre><code>##    user  system elapsed 
##   5.663   0.143   5.808</code></pre>
<p>So far we have been running all of our simulations in sequence. However, each simulation, by construction, is independent of the others. This is a perfect use case for parallelization.</p>
<p>There is a <code>mc_replicate()</code> helper function (originally from the <a href="https://github.com/rmcelreath/rethinking"><em>rethinking</em></a> package) that makes this easy for the code we have written so far.</p>
<pre class="r"><code>xfun::pkg_attach2(&quot;mcreplicate&quot;)

system.time(
  mc_replicate(n_sims, one_sim(n = n_obs, 
                              control_prob = control_prob, 
                              rel_effect = rel_effect),
              mc.cores = 4)
)</code></pre>
<pre><code>##    user  system elapsed 
##   4.634   0.191   1.636</code></pre>
<p>As we would expect from running the simulations across 4 rather than one core, the <code>elapsed</code> time is faster with <code>mc_replicate()</code>. Note it is not 4 times faster in this case, because there is overhead in setting up the parallelization. With more samples we the overhead as a proportion of total computation time will decrease and we will approach 4x computation time.</p>
<p>Finally, let‚Äôs see if the difference of means is a useful way of estimating the treatment effect for a binary outcome? Let‚Äôs run a few more simulations. We should see normally distributed difference of means centered on 0.001:</p>
<pre class="r"><code>diff_means_1000 &lt;- mc_replicate(1e3, one_sim(n = n_obs, 
                                      control_prob = control_prob, 
                                      rel_effect = rel_effect),
                              mc.cores = 4)</code></pre>
<pre class="r"><code># find mean of all simulations for plotting
mean_of_means &lt;- mean(diff_means_1000)

# plot simulations with line at mean value
diff_means_1000_tbl &lt;- as_tibble(diff_means_1000)

ggplot(diff_means_1000_tbl, aes(value)) +
  geom_histogram(alpha = 0.6, fill = &quot;#29ffc6&quot;) +
  geom_vline(xintercept = mean_of_means, linetype = &quot;dashed&quot;) +
  xlab(&quot;\nTreatment - Control&quot;) +
  ggtitle(&quot;Monte Carlo Results&quot;, 
          subtitle = &quot;Difference of means for binary outcome\n1,000 simulations&quot;)</code></pre>
<p><img src="/post/2021-03-11-simulation_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>In this example, the more simulations we run, the more smooth and ‚Äúnormal‚Äù the curve will become.</p>
</div>
</div>
<div id="debugging-intro" class="section level2">
<h2>Debugging in R</h2>
<p>Debugging is the process of finding and resolving discrepancies between a program‚Äôs specification and its implementation.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> For example, I want a ggplot2 to plot multiple time series on the same plot as separate lines, but it plots them all as one‚Äìvery jagged and confusing line. I could use a debugging process to find the cause of and correct this discrepancy.</p>
<p>The reason we are covering debugging now, is that statistical simulations are more complex computational and coding processes than we‚Äôve seen before, so debugging them is more complicated.</p>
<div id="being-mindful-of-your-feelings" class="section level3">
<h3>Being mindful of your feelings</h3>
<p>We‚Äôll cover the the technical debugging tools available in R. But I first want to first mention a crucial part of effective (and less stressful) debugging: <em>being mindful of your feelings about the bug</em>.</p>
<p>A bug is frustrating. Our first encounter with a bug is an experience of disassociation between how we think the world should work and how it is working. It‚Äôs as if we want to put a plate in the cupboard, but each time we try to put the plate in the cupboard it flies out. Frustration inclines us to look for the cause of a bug in‚Äìalmost always‚Äìthe wrong place: ‚Äúthe stupid computer is broken‚Äù.</p>
<p>The best way to resolve bugs, as we‚Äôll see, is to create testable hypotheses about what could be causing the bug. ‚ÄúThe stupid computer is broken‚Äù is vague and so not a testable hypothesis.</p>
<p>So, it is crucial that when you find a bug, to take a step back and ask:</p>
<blockquote>
<p>how does this bug make me feel?</p>
</blockquote>
<p>If the answer is ‚Äúangry, I want to throw my computer out the window‚Äù, your best next step is to <em>take a break</em>. Grab a tea or whatever. Then, with a piece of paper (or some other tool that gets you away from the temptation to just begin slamming out code on your keyboard) write down hypotheses about the cause of the bug and ways to test them.</p>
<p><strong>Unlike social phenomena, many computer bugs are deterministic</strong>. The cause of a bug will usually cause the bug. So your chances of definitively finding the bug are high with the following process and tools.</p>
</div>
<div id="clear-and-testable-program-specifications" class="section level3">
<h3>Clear and testable program specifications</h3>
<blockquote>
<p>Finding your bug is a process of confirming the many things that you believe are true ‚Äî until you find one which is not true.</p>
<p>‚ÄîNorm Matloff (quote from <a href="https://adv-r.hadley.nz/debugging.html#debugging-strategy">Advanced R</a>)</p>
</blockquote>
<p>Great debugging is driven by having great expectations of what the program should return. Having a well defined expectation of what a program should do is important to knowing if you have a bug to begin with. One way to approach software development is to write tests for a function <em>before</em> you write the function. This is called <a href="https://en.wikipedia.org/wiki/Test-driven_development">test driven development</a>.</p>
<p>Both in your functions, to test incremental expectations, and for the output of your functions you include automated tests. Here is a silly example. Imagine that we have a function that should add 2 + 2 and then divide by 2 to return 2. We can create an automated test like this:</p>
<pre class="r"><code># Function to test
two &lt;- function() {
  four &lt;- 2 + 2
  if (four != 4) 
    stop(&quot;Did not add 2 + 2 correctly&quot;)
  
  four / 2
}

# Automated test 
if (two() != 2) stop(&quot;function two did not return two&quot;)</code></pre>
<p>The <code>if</code> . . . <code>stop</code> statements automatically test whether or not the function is executing correctly. If there is a problem, the tests should help us pinpoint the cause more quickly.</p>
<p>Automated tests will also help us make sure that our code is correct even as we change it, for example, to add new features.</p>
</div>
<div id="browser-for-testing-and-dissecting" class="section level3">
<h3><code>browser()</code> for testing and dissecting</h3>
<p>A key debugging R tool is the <code>browser()</code> function. When R executes some code and hits <code>browser()</code> it stops the execution and begins an interactive <code>Browse</code> mode. This allows you to explore objects created in the code execution in the environment that created then, as well as run new code. RStudio works really well with this function.</p>
<p>Imagine we wrote the loop above incorrectly:</p>
<pre class="r"><code>one_sim_bug &lt;- function(n, control_prob, rel_effect) {
  treat_prob &lt;- control_prob / (control_prob * rel_effect)
    
  cy &lt;- rbinom(n = n, size = 1, prob = control_prob)
  ty &lt;- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}

mean_diff_bug &lt;- rep(NA, n_sims)

for (i in seq_along(mean_diff)) {
  mean_diff_bug[u] &lt;- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}</code></pre>
<p>We have two layers of nested functions where the bug could be. Let‚Äôs systematically use <code>browser()</code> to identify the source. Start from the outer most function‚Äìthe <code>for</code> loop and work you way down as necessary.</p>
<pre class="r"><code>for (i in seq_along(mean_diff)) {
browser()
  mean_diff_bug[u] &lt;- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}</code></pre>
<p>Ok, we‚Äôve found the typo. We should have used <code>i</code> instead of <code>u</code> to index the output of <code>one_sim</code>. To leave the browser environment type <code>c + Enter</code> (or on some Windows machines <code>Q + Enter</code>). Are we done? We‚Äôll let‚Äôs check the output:</p>
<pre class="r"><code>for (i in seq_along(mean_diff)) {
  mean_diff_bug[i] &lt;- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}

hist(mean_diff_bug)</code></pre>
<p>Hm, we are not getting a valid histogram. Let‚Äôs now go down the function nesting to insert a <code>browser()</code> call in <code>one_sim</code>:</p>
<pre class="r"><code>one_sim_bug &lt;- function(n, control_prob, rel_effect) {
  browser()
  treat_prob &lt;- control_prob / (control_prob * rel_effect)
    
  cy &lt;- rbinom(n = n, size = 1, prob = control_prob)
  ty &lt;- rbinom(n = n, size = 1, prob = treat_prob)
  
  mean(ty) - mean(cy)
}

mean_diff_bug &lt;- rep(NA, n_sims)

for (i in seq_along(mean_diff)) {
  mean_diff_bug[u] &lt;- one_sim_bug(n = n_obs, 
                                  control_prob = control_prob, 
                                  rel_effect = rel_effect)
}</code></pre>
<p>When we do this, we can find that we incorrectly divide <code>control_prob</code> by <code>(control_prob * rel_effect)</code> the ultimate result, is that the output of <code>one_sim</code> is <code>NA</code>. We can‚Äôt create a histogram of <code>NA</code>s. Once we solve this issue, we should get the expected histogram.</p>
</div>
</div>
<div id="exercise" class="section level2">
<h2>ü•Ö Exercise</h2>
<p><strong>A.</strong> Replicate the example above. This time use Monte Carlo simulation to find the distribution of p-values if you had a <em>total</em> sample size of 2,000. Hint: extract p-values in R from a t-test with <code>t.test()$p.value</code> .</p>
<p>‚ÄúExtra credit‚Äù: how often would you incorrectly reject the null hypothesis of no difference of means at the 95% level with <em>total</em> sample sizes of 1,000, 2,000, 10,000, and 100,000? Plot the proportion of simulations falsely rejected across these sample sizes.</p>
<p><strong>B.</strong> Imagine that we are interested in understanding how bad <a href="https://en.wikipedia.org/wiki/Confounding">confounding</a> by an unobserved variable could be in a DGP:</p>
<p><span class="math inline">\(Z\)</span> is a confounder. It causes both <span class="math inline">\(X\)</span> and our outcome of interest <span class="math inline">\(Y\)</span>. Imagine we couldn‚Äôt observe <span class="math inline">\(Z\)</span>. Create a Monte Carlo simulation to explore how this confounder could be biasing our estimate of the relationship between <span class="math inline">\(X\)</span> and our <span class="math inline">\(Y\)</span>.</p>
<p>Assume a data generating process described by:</p>
<p><span class="math display">\[
Z \sim \mathrm{Binomial}(0.5)\\
X \sim \mathrm{Binomial}(0.8 - 0.6 \cdot Z) \\
\epsilon \sim \mathrm{Normal}(0, 1) \\
Y = X \cdot 1 + Z \cdot 3 + \epsilon
\]</span></p>
<p>How different is the estimated effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> when we include the unobserved confounder compared to when we don‚Äôt include it?</p>
<p>Hints: use <code>lm()</code> to estimate a linear model. Coefficients can be extracted from <code>lm</code> model objects, say <code>m1</code> with <code>m1$coefficients</code>.</p>
<p>Extra credit: how does this impact change as you vary the relationship between <span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> i.e.¬†change <span class="math inline">\(0.6 \cdot Z\)</span>? Answering this question will likely take some programming beyond what we have covered so far. Google is your friend here.</p>
<p><strong>Extended pair project: Option 1</strong></p>
<ol style="list-style-type: decimal">
<li><p>Propose a data generating process and two estimation strategies for a quantity of interest from this DGP related to work that you are interested in.</p></li>
<li><p>Create ‚Äúacceptance criteria‚Äù to decide it which estimation strategy is more promising given this DGP.</p></li>
<li><p>Conduct Monte Carlo analysis of the estimation strategies and determine what is the best estimation strategy.</p></li>
</ol>
<p><strong>Extended pair project: Option 2</strong></p>
<p>This project requires significant additional Googling/questions for the instructor, but you will learn the valuable skill of creating R packages. R packages are a great way to wrap up reusable code, share this code with others, and ensure it has a high quality, for example by building in automated testing.</p>
<p>In this pair project:</p>
<ul>
<li><p>create an R package from the functions in this example. Hint: in RStudio click <code>New Project. . .</code> -&gt; <code>New Directory</code> -&gt; <code>R Package</code>.</p></li>
<li><p>ensure the functions are well documented using the function documentation syntax above.</p></li>
<li><p>create examples and automated tests to ensure that the functions run correctly using the <code>testthat</code> package. Hint: install the <em>usethis</em> package and, within your package‚Äôs R project enter the function <code>use_testthat()</code>.</p></li>
</ul>
<p>A great resource for R package creation is Wickham and Bryan‚Äôs <a href="https://r-pkgs.org/">R Packages</a>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gelmanhillaki2021" class="csl-entry">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. <em>Regression and Other Stories</em>. Cambridge: Cambridge University Press.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>This example is modified from <span class="citation">Gelman, Hill, and Vehtari (<a href="#ref-gelmanhillaki2021">2021</a> Ch. 5, p.69-70)</span>.<a href="#fnref1" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>We can express this in R using scientific notation, e.g.¬†<code>2e6</code>.<a href="#fnref2" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>These conventions are established by the <a href="https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html">roxygen2</a> package. If you create an R package to contain your functions, you can use roxygen2 to automatically generate package documentation. For more details see <a href="https://r-pkgs.org/">R Packages</a>.<a href="#fnref3" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p>If you ever need to kill an R process (e.g.¬†because it is taking way longer than you expected, use <code>Ctrl + c</code> in your R console.<a href="#fnref4" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn5"><p><code>replicate()</code> is a wrapper for <code>sapply()</code> you can see how this code works by typing <code>sapply</code> into your R console.<a href="#fnref5" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn6"><p>Note that there is random variation in computation time. So if you want to get a more accurate sense of computation time use the <a href="https://cran.r-project.org/web/packages/microbenchmark/microbenchmark.pdf">microbenchmark</a> package. This does repeated runs of the call and reports the distribution of run times.<a href="#fnref6" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn7"><p>See the <a href="https://en.wikipedia.org/wiki/Debugging">Wiki page on debugging</a> for a visual example of an early computer bug, an actual bug that got stuck in a computer.<a href="#fnref7" class="footnote-back">‚Ü©Ô∏é</a></p></li>
</ol>
</div>
